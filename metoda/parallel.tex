% Copyright (c) 2016 Grigory Rechistov <grigory.rechistov@gmail.com>
% This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Worldwide.
% To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/.

\chapter{Параллельные симуляторы}\label{parallel}

% It is pitch dark. You are likely to be eaten by a grue

\dictum[Хорхе Луис Борхес. Сад расходящихся тропок]{В  отличие  от  Ньютона  и Шопенгауэра  ваш предок не верил в единое, абсолютное время. Он верил   в   бесчисленность   временных   рядов,  в   растущую, головокружительную сеть расходящихся, сходящихся и параллельных времён.}

Некоторое время назад неограниченный рост частоты процессоров прекратился по различным причинам, в основном связанным с невозможностью удерживать их тепловыделение в допустимых пределах без специальных ухищрений, увеличивающих стоимость продуктов и уменьшающих их применимость для встраиваемых и мобильных решений. Силы проектировщиков микросхем обратились к другому способу, потенциально позволяющему повысить производительность — увеличивать количество независимых вычислительных ядер в составе вычислительного комплекса. Оставляя за рамками обсуждения вопросы целесообразности, эффективности и программируемости таких систем, рассмотрим, как распространение многопроцессорных систем влияет на постановку и решения задачи компьютерной симуляции.

\begin{enumerate*}
\item \textit{Необходимость моделирования многопроцессорных систем.} Как было рассмотренно в предыдущих главах, возможно моделировать многие устройства в одном потоке исполнения, перемежая их исполнение и следя за тем, чтобы разница в виртуальном времени не превышала допустимых для симуляции пределов. Однако при этом мы сталкиваемся с тем, что время исполнения модели растёт линейно с числом последовательно моделируемых, и скорость симуляции быстро становится неприемлемо малой.

\item \textit{Возможность эффективного задействования многопроцессорных ресурсов хозяйской системы.} Если есть возможность задействовать все доступные ресурсы аппаратуры для работы программы и при этом получить ускорение, то это необходимо сделать.
\end{enumerate*}

\paragraph{Замечания о терминологии.} Всюду в данной главе мы будем отвлекаться от деталей иерархической организации многопроцессорных систем и реализаций систем памяти. При этом термин <<процессор>> будет использоваться для обозначения устройства, исполняющего ровно один поток последовательных инструкций и имеющий одну копию архитектурного состояния. Также, если это явно не будет оговорено, понятия <<процесс>> и <<поток>> исполнения будут использоваться взаимозаменяемо. 

\section{Последовательные модели}

Напомним кратко принцип работы двух последовательных однопоточных схем, рассмотренных ранее: симуляция многопроцессорных систем и модель дискретных событий. Затем попытаемся сформулировать способы их превращения в параллельные схемы, способные задействовать более одного хозяйского потока для симуляции, и рассмотрим возникающие при этом сложности.

\subsection{Симуляция нескольких гостевых процессоров}

В главе~\ref{fullplatform} был описан способ моделирования многопроцессорных систем в однопоточной симуляции: отдельные процессоры выполняются последовательно друг за другом, выдерживая определённый максимально допустимый разброс в значениях симулируемого времени. Остальные процессоры при этом <<заморожены>>. Очевидно, что для такого алгоритма работы для модели системы $N$ процессоров относительное замедление относительно модели с одним процессором составит в лучшем случае $N$; на практике оно будет больше из-за необходимости периодического переключения контекста — состояния модели от одного процессора к другому.

\subsection{Дискретные события}

При использовании модели DES имеется одна очередь событий, которую используют все моделируемые устройства. Скорость симуляции определяется количеством и сложностью обработки отдельных элементов этой очереди. С ростом числа устройств она неизбежно падает. Растут также требования на другие ресурсы системы, такие как память, требуемая для хранения элементов очереди и архитектурного состояния.

\section{Параллельные модели}

Первый шаг к созданию параллельной симуляции — это распределение работы на несколько независимых потоков. Каждый из них выполняет свою часть работы и некоторым образом способен взаимодействовать с остальными потоками той же симуляции. 

Обрисуем достаточно общий и одновременно важнейший с практической точки зрения сценарий симуляции, требующий параллельного исполнения. На рис.~\ref{fig:parsim-overview} приведена общая схема распределения модели SMP-системы на несколько потоков. Каждый из них содержит группу моделируемых устройств, включая процессоры и периферию, имеет собственную очередь сообщений для хранения информации о запланированных событиях и может взаимодействовать с другими потоками (пока что не будем специфицировать точный механизм коммуникаций). Особое место в этой картине занимает моделируемая память — она общая для всей симуляции, чтение и запись её может происходить из любого моделируемого процессора или устройства.

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/parsim-overview}
    \caption[Общая схема распределения симуляции на несколько потоков]{Общая схема распределения симуляции на несколько потоков. Устройства и процессоры содержатся целиком внутри одного потока, тогда как модель памяти необходимо сделать доступной для всех}
    \label{fig:parsim-overview}
\end{figure}

\paragraph{Симуляция многопроцессорной системы.} Отдельные потоки в этом случае содержат один или более гостевой процессор и исполняют его инструкции, модифицирующие память и вызывающие методы периферийных устройств, разделяемых с остальными гостевыми процессорами, заключёнными в своих потоках. Каждый поток исполнения при этом самостоятельно ведёт счёт исполненных шагов симуляции, т.е. количества завершённых инструкций.

\paragraph{PDES.} При построении параллельной модели дискретных событий мы получаем так называемую схему parallel DES (PDES)~\cite{fujimoto-parallel-dist-sim, Fujimoto-pdes, Liu09paralleldiscrete-event, ferscha-1995-pdes}. При этом с каждым потоком исполнения ассоциируется собственная очередь дискретных событий, текущее значение симулируемого времени и архитектурное состояние одного или нескольких моделей устройств. Поскольку два взаимодействующих устройства могут оказаться в разных потоках, необходимо предусмотреть возможность создавать события в очередях, отличных от той, в которой находится порождающее событие.

Подчеркнём ещё раз важное по сравнению с последовательным вариантом изменение — каждый поток параллельной системы имеет своё значение симулируемого времени, которое используют заключённые в нём устройства. Разброс значений этих времён между потоками в общем случае не ограничен сверху заранее известной величиной. По этой причине обе описанные выше схемы не будут корректно работать по ряду причин, которые мы рассмотрим ниже. И, даже если предприняты меры по устранению таких причин, не следует ожидать линейного роста производительности с увеличением числа задействованных потоков из-за потерь на синхронизации различного рода, которые также будут рассмотрены.

\section[Препятствия параллельной модели]{Препятствия на пути к созданию корректной параллельной модели}

Параллельное программирование в целом является на удивление сложным занятием по сравнению с написанием классических последовательных программ. Этому есть много причин, вызванных техническими трудностями проектирования и психологическими особенностями человеческого восприятия, которые достаточно подробно рассмотрены в обширной литературе~\cite{Herlihy:2008:AMP:1734069, toporkov2004, Andrews:1999:FPD:519301}. Здесь лишь отметим четыре обстоятельства, важных для дальнейшего обсуждения и напрямую влияющих на задачу проектирования симуляторов.

\begin{enumerate*}
    \item Возможность гонок данных (\abbr data race) при одновременном доступе к общим данным и необходимость использования различных механизмов синхронизации, чтобы избежать их.
    \item Возможность взаимоблокировок двух или более процессов, пытающих получить доступ к общим ресурсам.
    \item Неэффективная работа параллельного приложения из-за интенсивной или неправильной синхронизации его потоков, из-за чего значительная часть их простаивает, не выполняя полезной работы.
    \item Результат исполнения параллельного приложения при идентичных входных данных может отличаться при различных его запусках из-за неопределённости порядка исполнения отдельных потоков. Недетерминистичность значительно усложняет отладку приложений.
\end{enumerate*}

Далее в этой главе рассматривается, как эти проблемы и особенности могут проявиться при создании и работе параллельных симуляторов.

\subsection[Атомарные инструкции]{Атомарные инструкции в моделях многопроцессорных систем}\label{sec:atomics}

Для обеспечения работы примитивов синхронизации параллельных программ современные процессоры предоставляют так называемые атомарные инструкции, при исполнении которых гарантируется, что чтение и/или модификация региона общей памяти, указанной в них, не будет пересекаться с доступом к той же памяти другими потоками. Существует множество вариантов этих инструкций в разных архитектурах~\cite{habr-atomics, habr-lock-free}. 

В последовательном симуляторе атомарность гостевых инструкций\footnote{Вообще всех инструкций, даже тех, которые в реальности не являются атомарными и могут участвовать в гонках данных.} обеспечивается автоматически: в любой момент времени максимум один процессор активен, никто не может повлиять на результат исполнения. Корректная поддержка этих инструкций в случае параллельного симулятора требует явных усилий со стороны разработчика. Рассмотрим известные для этого способы.

\paragraph{Использование атомарных хозяйских инструкций.} Идея использовать существующие хозяйские атомарные инструкции для симуляции атомарных гостевых достаточно проста. Она позволяет переложить задачу обеспечения корректной синхронизации с программы на аппаратуру. К сожалению, применимость этого способа ограничена случаями, когда набор атомарных инструкций хозяина достаточен для реализации всех инструкций гостя. Например, IA-32 содержит более десятка инструкций, которые можно использовать с префиксом \texttt{LOCK}, т.е. атомарных, тогда как ARM имеет только две инструкции данного типа — \texttt{LDREX} и \texttt{STREX}\footnote{Атомарные инструкции \texttt{SWP} и \texttt{SWPB}, присутствующие в ARMv5, объявлены устаревшими в последующих расширениях~\cite{arm-atomics}.}. Несомненно, этот подход применим для случаев совпадения архитектур хозяина и гостя~\cite{lantz-thesis}.

\paragraph{Использование критических секций.} Следующим достаточно простым возможным решением проблемы обеспечения эксклюзивного доступа к памяти при симуляции гостевых атомарных операций является использование критических секций (или семафоров, замков, мьютексов и т.п.) в симуляторе. Прежде чем исполнять часть гостевой инструкции, модифицирующую память, соответствующий поток симулятора обязан получить эксклюзивное право на модификацию региона памяти, содержащего операнд~\cite{pqemu2001}. Этим регионом может выступать вся физическая память (в таком случае есть только один замок на все потоки) или её блок, например, страница.

Тем не менее, использование критических секций для симуляции \emph{только} атомарных инструкций оказывается недостаточно для корректной работы гостевых приложений. В~\cite{coremu} приводится пример практически важного гостевого сценария, приводящего к некорректной блокировке при исполнении на симуляторе. Это происходит из-за того, что обычные, не атомарные, гостевые доступы в память не требуют вхождения в критическую секцию и тем самым способны создать гонку данных при одновременной симуляции с атомарными. Решение — обязать \emph{все} обращения в симулируемую память использовать вход в критическую секцию. Однако такое решение практически сведёт на нет весь выигрыш от параллельного исполнения, т.к. обращения к памяти из разных потоков приложения очень часты на практике.

\paragraph{Использование операций compare-and-swap и load-linked/ store-conditional.}  Проанализируем оба описанных выше приёма ещё раз и попытаемся наметить путь к общему решению.

Если удастся выразить все существующие атомарные инструкции несколькими универсальными операциями, то для хозяйских архитектур, реализующих этот базовый набор, удастся симулировать все инструкции гостевых систем. В работе по теоретическим основаниям параллельных алгоритмов~\cite{Herlihy:1991:WS:114005.102808} показано, что существующие синхронизационные примитивы могут быть реализованы с помощью атомарной операции <<сравнить и обменять значения>> (\abbr compare-and-swap, CAS). Однако также показано, что алгоритмы для некоторых структур данных, использующие CAS, могут работать некорректно\footnote{Так называемая <<проблема ABA>>~\cite{aba-problem}.}. Решением является использование расширенной операции, известной как <<сравнение и обмен двух ячеек>> (\abbr double compare-and-swap, DCAS). Однако ни одна современная архитектура не имеет машинных инструкций, соответствующих DCAS.

Следующий вариант — использование пары инструкций, называемых load-link и store-conditional (LL/SC)~\cite{jensen:load-linked}, позволяющих проверить, что цикл <<загрузить-изменить-сохранить>> для некоторого адреса в памяти прошёл без внешнего вмешательства, и сообщить об успехе или неудаче, что позволит повторить попытку провести операцию. Использование этой пары инструкций позволяет реализовать алгоритмы DCAS, CAS и другие примитивы.

Указанная возможность проверить успех завершения гостевой атомарной операции и в случае неудачи повторять блок симулирующего её кода является ответом также и на вторую проблему, связанную с небезопасностью использования критических секций только для атомарных операций.

Использование LL/SC можно считать частным случаем так называемой транзакционной памяти для оптимистичной синхронизации. %Подход с использованием транзакций будет более подробно разобран далее в этой главе для моделей PDES.

\subsection{Модели консистентности памяти}

Кроме обеспечения атомарности исполнения для подмножества инструкций, современные архитектуры накладывают определённые условия на то, в каком порядке могут быть видны все обращения в память из разных процессоров, определяя \textit{модель консистентности памяти}. Она описывает, насколько свободно аппаратура может переставлять чтения и записи как одного, так и нескольких потоков для того, чтобы увеличить скорость работы системы в целом\footnote{Отметим, что даже для однопроцессорной системы порядок доступов в память может не совпадать с программным как из-за влияния компилятора, переставившего их на этапе компиляции, так и аппаратуры, динамически определяющей порядок исполнения команд.}. Модели консистентности характеризуются своей <<силой>>, т.е. тем, насколько строго должен соответствовать наблюдаемый порядок доступов описанному в программе. Чем слабее модель, тем более <<вольно>> они могут быть переставлены, тем больше может быть выигрыш в производительности. Однако работа такой системы происходит менее интуитивно понятно с точки зрения программиста.

Подробное рассмотрение существующих моделей консистентности памяти выходит за рамки данной главы. Интересующийся читатель может найти подробную информацию об этой теме в обширной литературе~\cite{Adve96sharedmemory},~\cite{Mosberger93memoryconsistency},~\cite[глава 9 и приложение A.7]{DBLP:books/daglib/0013597},\cite{whymb}. Тем не менее, различия в используемых моделях могут повлиять на корректность симуляции в случае, если гарантии хозяйской системы слабее тех, которые требуются для работы гостевого окружения. 

Для целей контроля над порядком исполнения в точках приложения, для которых такой порядок критически важен, все современные архитектуры предоставляют так называемые инструкции-\textit{барьеры} (\abbr fence), которые гарантируют, что на момент их завершения все доступы в память определённого типа (чтения и/или записи) или завершились (для инструкций, предшествующих барьеру), или ещё не начались (для находящихся после неё в потоке исполнения). Для IA-32 это инструкции \texttt{LFENCE}, \texttt{SFENCE}, \texttt{MFENCE}~\cite{intel-64-memory-ordering}, для IA-64 — \texttt{mf}~\cite{itanium-mem-order}, для ARM — \texttt{DMB}, \texttt{DSB}, \texttt{ISB}, для PowerPC — \texttt{sync}, \texttt{eieio}.

Отметим, что без достаточно точной модели протокола передачи сообщений между процессорами и элементами подсистемы памяти невозможно обеспечить строгое выполнение модели консистентности гостевой системы. Однако функциональные модели могут быть написаны и корректно работать, если они обеспечивают более сильную модель, что всегда может быть достигнуто с помощью барьеров.

\subsection[Нарушения каузальности]{Нарушения каузальности}

Рассмотрим ещё одну проблему, подстерегающую на пути к построению параллельной модели, на примере модели дискретных событий с несколькими очередями, одновременно изменяющими своё состояние. Нарушение требования соответствия порядка событий в симулируемой системе относительно их порядка в системе реальной проиллюстрируем следующим образом.

% На рис.~\ref{fig:two-queues-1} два потока производят коммуникации, посылая друг другу сообщения в виде событий, помещаемых в очередь и запланированных на исполнение в некоторые моменты в будущем относительно симулируемого времени отправителя. Предположим, что в некоторый момент физического времени по каким-то причинам поток-отправитель имеет симулируемое время $T_2$ большее, чем первый $T_1$ (например, события в нём происходят реже, и потому он продвигается быстрее).
% 
% \begin{figure}[htbp]
%     \centering
%     %\includegraphics[width=\textwidth]{./two-queues-1-crop.pdf}
%     \begin{tikzpicture}[>=latex, font=\scriptsize]
%     
%     \draw[->] (0,0) -- (10,0) node[pos=0.9, below] (sim-time1) {Время};
%     \foreach \x in { 1, 2, 3, 4, 5, 6, 7, 8, 9} { 
%         \draw (\x,-0.15) -- (\x,0.15) node (tick\x) {};
%     };
%     
%     \node[draw, arrow box, arrow box arrows={north:.7cm}] at (2, -1) {$T_1$};
%     \node[draw, arrow box, arrow box arrows={south:.6cm}, text width = 0.6cm] at (2, 1) {};
%     
%     \node[dashed, draw, arrow box, arrow box arrows={south:.6cm}, text width = .6cm] at (5, 1) {};
%     \node[draw, arrow box, arrow box arrows={south:.6cm}, text width = .6cm] at (8, 1) (ev2) {};
%     
%     \draw[->] (0,-3) -- (10,-3) node[pos=0.9, below] (sim-time2) {Время};
%     \foreach \x in { 1, 2, 3, 4, 5, 6, 7, 8, 9} { 
%         \draw (\x,-3.15) -- (\x,-2.85) node (tick\x) {};
%     };
%     
%     \node[draw, arrow box, arrow box arrows={north:.7cm}] at (4, -4) {$T_2$};
%     \node[draw, arrow box, arrow box arrows={south:.6cm}, text width = .6cm] at (4, -2) (ev1) {};
%     
%     \draw[->] (ev1.east) .. controls +(3,0) and +(-2,0) .. (ev2.west);
%     
%     
%     \end{tikzpicture}
%     \caption{Нарушение корректности симуляции. Случай $T_2 > T_1$. Штрихами показано правильное положение создаваемого события, сплошной линией — некорректное}
%     \label{fig:two-queues-1}
% \end{figure}

% Тогда при помещении события в очередь оно окажется дальше в будущем, чем должно было бы быть. 

На рис.~\ref{fig:two-queues-2} два потока взаимодействуют, посылая друг другу сообщения в виде событий, помещаемых в очередь и запланированных на исполнение в некоторые моменты в будущем. При некотором соотношении текущих симулируемых времён $T_1$ и $T_2$ может случится так, что добавляемое из другой очереди событие окажется в прошлом для получателя.

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/two-queues-2}
    \caption[Нарушение корректности относительного положения событий]{Нарушение корректности относительного положения событий в случае $T_2 < T_1$. Новое событие оказалось в прошлом и не может быть обработано}
    \label{fig:two-queues-2}
\end{figure}

В реальной системе новое событие в обоих случаях получило бы строго одно и то же положение и относительный порядок во времени; при наивной параллельной симуляции возникают каузальные ошибки (нарушение причинно-следственной связи событий).

Отметим, что соотношение между $T_1$ и $T_2$ может быть произвольным по многим причинам, чаще всего не контролируемым пользователем. Например, хозяйская операционная система может решить вытеснить (т.е. временно заморозить) один из потоков, может случиться промах страницы в виртуальной памяти, промах в системе кэшей и т.п. Поэтому такая ситуация, когда один поток исполняется быстрее остальных и имеет значение симулируемого времени, значительно отличающееся от остальных, более чем вероятна.

% Отметим, что в рассмотренной ранее схеме однопоточной многопроцессорной симуляции нарушение причинно-следственной связи также возможно в случае численных  значений квоты, превышающих один такт. Однако при этом ситуация не безвыходная, так как ошибочное поведение будет сохраняться при всех запусках, и, обнаружив его, мы всегда можем исправить ситуацию, уменьшая значение кванта времени одного процессора, вплоть до минимума в один шаг, гарантирующего правильность. В параллельной системе такая ошибка будет неуловимой и проявляющейся лишь иногда (и по-разному) из-за неустранимой недетерминированности относительных скоростей симуляции в разных потоках при отдельных запусках; более того, при подключении отладчика мы, скорее всего, своим вмешательством будем упорядочивать исполнение потоков, маскируя ошибку полностью.

Что же делать? Прежде всего, достаточно легко сформулировать, как распознавать ситуацию нарушения каузальности и корректировать её для первого случая $T_2 > T_1$. Достаточно к сообщениям об обращении к разделяемому состоянию добавлять метку времени того события, которое производит доступ. Поток, получающий сообщение (или использующий модифицированные  данные), сравнивает значение этой метки со своим локальным временем, корректирует момент обработки события, а при обнаружении логического несоответствия (например, если новое событие оказывается в прошлом) сигнализирует о проблеме (рис.~\ref{fig:timestamps}). Конечно, этот метод не позволяет разрешать возникшее затруднение, но только обнаруживать его. 

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/timestamps}
    \caption[Пересылка меток времени создания событий вместе с самим событием]{Пересылка меток времени создания событий вместе с самим событием. Поток-приёмник корректирует положение события и принимает решение о том, соответствует ли ситуация условиям корректной симуляции}
    \label{fig:timestamps}
\end{figure}

\paragraph{Пути решения проблемы нарушения каузальности}

Существует два принципиально различных подхода проектирования, способных помочь нам.

\begin{enumerate*}
    \item Расширить схему PDES таким образом, чтобы в принципе \emph{не допускать} каузальных ошибок. Такие системы назовём \emph{консервативными}. 
    \item \emph{Детектировать} нарушения уже после их возникновения, а затем \emph{исправлять} ситуацию, возможно, с помощью частичного перезапуска симуляции. Назовём этот класс \emph{оптимистические} системы.
\end{enumerate*}

\section[Консервативные модели]{Консервативные модели}

Мы хотим исключить возможность возникновения ситуаций, когда сообщение от потока, отставшего в симулируемом времени, приходит другому потоку «в прошлое». Предлагаемое решение — придержать (блокировать) исполнение отправителя сообщения до тех пор, пока приёмник сам не продвинется в симулируемом времени до точки приёма~\cite{Misra86distributeddiscrete-event}. Пример такой ситуации изображён на рис.~\ref{fig:send-and-block}. Очевидно, что блокировать процесс следует, если он посылает новое событие в чужую очередь, но не в собственную — в этом случае порядок нарушиться не может. Описанная схема позволяет добиться корректности с помощью того, что потоки, вырывающиеся вперёд, вынуждены впоследствии ожидать более медленных.

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/send-and-block}
    \caption[Консервативный сценарий параллельной симуляции]{Консервативный сценарий параллельной симуляции. Поток, желающий добавить событие в очередь другого потока, блокируется до момента обработки этого события}
    \label{fig:send-and-block}
\end{figure}


\subsection{Необходимость предпросмотра}

Предпросмотр (\abbr lookahead) — определение того, на какое значение продвижение виртуального времени потока безопасно, т.е. не может вызвать нарушения каузальности. Чем больше это значение, тем большую <<независимость>> имеют отдельные потоки симуляции в своей работе, и тем выше её производительность в целом. Чрезмерно большое значение этой величины будет вызывать нарушения в логике  работы гостевых приложений. Наиболее естественный выбор значения для предпросмотра равен характерной задержке передачи сообщений между устройствами в реальной системе. Необходимость использования предпросмотра для консервативных схем~\cite{ferscha-1995-pdes} обусловлена невозможностью узнать состояние других потоков без получения от них сообщений.

\subsection{Проблема взаимоблокировок}

К сожалению, такая схема совсем не гарантирует, что сообщения в системе будут передаваться и симуляция будет прогрессировать~\cite{Misra86distributeddiscrete-event} — возможна ситуация взаимоблокировки (\abbr deadlock). На рис.~\ref{fig:deadlock} приведён пример такой ситуации.

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/deadlock}
    \caption[Ситуация взаимной блокировки в системе с тремя потоками]{Ситуация взаимной блокировки в системе с тремя потоками. Каждый процесс ожидает сигнала окончания блокировки от какого-либо другого потока}
    \label{fig:deadlock}
\end{figure}

В случае большого числа потоков в симуляции взаимоблокировка может затронуть только часть из них, при этом остальные будут продолжать исполняться до тех пор, пока не посылают сообщения к заблокированной группе. Как разрешать сложившуюся ситуацию? Если уметь обнаруживать взаимоблокировку, то возможно принудительно освободить один из участвующих в ней поток, разрешив ему исполняться. Это не нарушит условий корректности симуляции. Для определённости освобождать будем тот процесс, который имеет наименьшее значение текущего симулируемого времени.  На рис.~\ref{fig:deadlock} им будет поток номер 1. Его разблокировка также оправдана с точки зрения выдерживания наименьшего разброса значений времён в разных потоках. Продвижение освобождённого потока в конечном счёте активирует и другие процессы, избавив их от блокировок, и симуляция продолжится в полном объёме.

Как можно детектировать ситуацию взаимоблокировки? В литературе описывается ряд алгоритмов, некоторые подразумевают присутствие центрального наблюдателя, некоторые являются децентрализованными. Кратко рассмотрим алгоритм, предложенный в~\cite{Misra86distributeddiscrete-event}. Он основан на  передаче специального сообщения (маркера) между процессами. Каждый процесс обязан передать его в течение ограниченного времени. В состоянии маркера хранится количество и состояния (блокирован или обрабатывает сообщения) посещённых процессов. Если в некоторый момент число обнаруженных маркером блокированных процессов достигает критического значения, то объявляется обнаружение ситуации взаимоблокировки.

Как корректно разрешить обнаруженную взаимоблокировку? Для этого достаточно освободить один из процессов, участвующих в ней. Какой из многих выбрать? Следует выбирать поток, значение симулируемого времени которого минимально — это позволит ему обработать сообщения от остальных заблокированных потоков, таким образом освободив их и позволив всей глобальной симуляции продвигаться. 

Можно ли полностью исключить ситуацию блокировок? Поскольку сам факт их возникновения связан с тем обстоятельством, что отдельные потоки не имеют знания о симулируемом времени в остальных потоках, необходимого, чтобы самостоятельно принять решение, безопасно ли им продвигать вперёд своё время и посылать сообщения другим процессам, не опасаясь нарушения причинной связи. Как было описано ранее, метки времени приходят вместе с сообщениями. Но что делать, если архитектурных событий, вызывающих сообщения, не предвидится? В таком случае можно отправлять \textit{пустые сообщения} (\abbr null messages), несущие только метку времени. Они должны периодически (в терминах физического времени) рассылаться и получаться всеми агентами внутри одной симуляции; при приёме очередной метки поток может оценить, до какого момента следует продвигать своё время без опасности при этом послать некорректное сообщение. Блокировка всех потоков исключена, т.к. самый медленный поток не блокируется.

Примечательное следствие из этого обстоятельства состоит в следующем: даже если по архитектурным причинам все потоки заблокировались (например, все процессоры перешли в выключенное состояние и больше не создают новые события в своих очередях), периодическая отсылка пустых сообщений будет вынуждать их продвигать локальное симулируемое время.

Как часто необходимо слать пустые сообщения? При частой отправке значительная доля времени работы потока-отправителя тратится не на симуляцию, а на синхронизацию. При редких синхронизациях потоки-получатели будут простаивать, ожидая сигналов о безопасности собственного продвижения вперёд.

Следующий аспект функционирования такой схемы — в каком порядке  и каким адресатам должны слаться пустые сообщения. Это можно делать несколькими способами, например следующими.

\begin{description*}
\item[Всем агентам в системе.] При таком сценарии мы обеспечиваем наиболее актуальную информацию всем потокам о глобальном симулируемом времени. Однако с ростом их числа эта схема плохо масштабируется из-за квадратичного роста числа передаваемых сообщений.

\item[Случайным адресатам]~\cite{graphite2010}. При таком подходе отправитель каждый раз выбирает случайным образом небольшую долю от полного числа потоков для сообщения им своего состояния. Благодаря постоянно изменяющемуся списку адресатов можно ожидать, что информация о каждом потоке будет постепенно распространяться по всей системе за конечное время.

\item[Запрос получателем.] Поток, участвующий в симуляции, при необходимости узнать значение виртуального времени другого потока может сам явным образом запросить его, не ожидая ближайшей рассылки. Преимущество данного способа — пустые сообщения передаются только тогда, когда они действительно запрошены. Недостаток связан в необходимости передачи двух сообщений <<запрос — ответ>>, что занимает в два раза больше времени.
\end{description*}

Отметим, что схема, в которой каждый поток периодически шлёт пустые события только фиксированному набору адресатов, не гарантирует невозможность возникновения в ней неразрешимых взаимоблокировок.

\section{Оптимистичные модели}

\dictum[Postal 2]{Эту игру смогла бы пройти и моя бабка, если бы она сохранялась столько же, сколько и ты!}

На практике события нарушения каузальности могут происходить достаточно редко. Тем не менее, консервативный алгоритм будет блокировать потоки каждый раз, когда потенциально возможно получить их рассинхронизацию, тем самым снижая эффективность параллельной симуляции. Вычислить заранее, действительно ли блокировка будет необходима, затруднительно (см. секцию~\ref{sec:why-pdes-hard}).

Поступим иначе. Во-первых, некоторым образом будем сохранять информацию о моментах в симулируемом прошлом. Необходимо иметь возможность восстановить состояние модели к каждому из выбранных моментов, не начиная симуляцию с самого начала. Назовём такой набор данных \textit{точкой сохранения} (\abbr checkpoint). В простейшем случае точка сохранения просто содержит значения всех архитектурных состояний всех устройств, входящих в симуляцию, а также значение симулируемого времени.

Во-вторых, позволим параллельной симуляции исполняться без блокировок потоков. При этом возможны нарушения каузальности, поэтому необходимо при передаче сообщений проверять метки их времени. Что же делать, если такое нарушение было обнаружено? В этому случае состояние модели восстанавливатеся из одной из точек сохранения, про которую известно, что на момент её создания нарушений обнаружено не было. Затем симуляция «опасного» участка производится повторно оптимистичным образом или с использованием других описанных ранее схем, см. рис.~\ref{fig:checkpoints}.

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/checkpoints}
    \caption[Симуляция с периодическими точками сохранения]{Симуляция с периодическими точками сохранения. Штрихами показана часть симуляции, приведшая к нарушению корректности и откату к ближайшему сохранённому состоянию}
    \label{fig:checkpoints}
    % \begin{tikzpicture}[>=latex]
    % \def\arrow{ % taken from http://www.texample.net/tikz/examples/overlapping-arrows/
      % (10.75:1.1) -- (6.5:0.8) arc (6.25:120:0.8)  --
      % (120:0.7)  -- (130:1.1)  --
      % (120:1.4)  -- (120:1.2) arc (120:5.25:1.2)
       % -- (10.75:1.1) -- (6.5:1)
    % }
    % % \node[circle, inner sep=1pt, draw] {};
    % \draw +(-0.5,-0.75) -- +(0.5,-0.75) -- +(0.7,0.75) -- +(-0.7,0.75) -- +(-0.5,-0.75);
    
    % \begin{scope}[scale=0.4]
        % \draw \arrow;
        % \draw[rotate=120] \arrow;
        % \draw[rotate=240] \arrow;
    % \end{scope}    
    % \end{tikzpicture}
\end{figure}

Очевидно, что выигрыш в производительности от использования оптимистичной схемы возникает в предположении, что нарушения каузальности и связанные с ними откаты состояния будут нечастыми. На производительность влияют и другие аспекты схемы.

\begin{description*}
    \item[Цена создания точек сохранения.] Для сохранения состояния симуляции требуется как место в памяти хозяина, так и некоторый интервал хозяйского времени, в течение которого вся симуляция остановлена. Способ минимизировать обе величины — сохранять в новой точке лишь изменения в состояниях устройств относительно предыдущей, т.е. использовать \textit{инкрементальные} точки сохранения.
    \item[Частота создания точек сохранения.] Чем ближе к потенциальному месту нарушения имеется записанное состояние симуляции, тем меньше шагов необходимо переисполнять в случае отката.
    \item[Стоимость отката состояния.] Как и создание, откаты можно сделать инкрементальными, то есть затрагивающими лишь те части симуляции, которые изменились с момента последнего сохранения.
\end{description*}

Разработаны многочисленные варианты оптимистичных алгоритмов параллельной дискретной симуляции. Рассмотрим наиболее известный протокол, получивший название Time Warp\footnote{Его авторы описывают свой протокол как <<виртуальное время>> по аналогии с существующим понятием <<виртуальная память>>.}~\cite{virtual-time}.

\subsection{Time Warp}

Определим основные понятия, используемые при описании алгоритма Time Warp.

\begin{itemize*}
    \item Сообщение — набор данных, описывающих событие, которое должно быть добавлено в одну из очередей событий. Оно характеризуется, кроме своего непосредственного содержимого, виртуальными временами отправки $t^{send}$ и обработки $t^{receive}$.
    \item $LVT$ (\abbr local virtual time) — значение симулируемого времени отдельного потока, участвующего в симуляции. Для создаваемых событий их время отправки $t^{send}$ равно значению $LVT$ отправителя. В отличие от консервативных схем, эта величина может как расти в процессе симуляции, так и убывать в случае отката процесса.
    \item $GVT$ (\abbr global virtual time) — глобальное время для всей симуляции, определяющее, до какой степени возможно её откатывать.  Глобальное время всегда монотонно растёт, всегда оставаясь позади локального времени самого медленного потока, а также оно меньше времени отправки самого раннего ещё не доставленного события:
    $$GVT \leq \min \left( \min\limits_{i} LVT_{i}, \min\limits_{k} t^{send}_k  \right).$$
    \item Отставшее сообщение (\abbr straggler) — событие, пришедшее в очередь с меткой времени $t^{receive}_{straggler}$, меньшей, чем $LVT$ получателя. Его обнаружение вызывает откат текущего состояния, при этом $LVT$ уменьшается, пока не станет меньше, чем $t^{receive}_{straggler}$, после чего оно может быть обработано. После этого возобновляется прямая симуляция.
    \item Антисообщение (\abbr antimessage) — механизм обеспечения откатов в Time Warp. Каждое антисообщение соответствует одному ранее созданному сообщению, порождённому в интервале симулируемого времени $[t_{straggler}, LVT_i]$ и  вызывает эффект, обратный его обработке (т.е. возвращает состояние в исходное). Поток, обнаруживший прибытие в свою очередь отставшего сообщения, при своём откате рассылает антисообщения всем потокам, с которыми он успел провзаимодействовать, таким образом сообщая о том, что необходимо отменить часть их прямой симуляции, на которую он успел повлиять. Каждый получатель запроса на отмену исполняет его, тем самым уничтожая эффекты от предыдущего события. Если же получатель обнаружит, что антисообщение имеет метку времени меньше его $LVT$ (т.е. оно для него является отставшим), то он также производит откат, рассылая новые антисообщения. Благодаря этому последствия некорректной симуляции постепенно отменяются глобально.
    
    \item Сбор окаменелостей (\abbr fossil collection) — своеобразное название механизма сбора мусора (\abbr garbage collection). Как будет показано дальше, из всех обработанных всеми потоками событий безопасно удаляемы только те, что имеют метку времени, меньшую чем $GVT$. Для того, чтобы объём потребляемой при симуляции памяти не рос безгранично, необходимо регулярно её освобождать для последующего переиспользования. Непосредственно сбором окаменелостей может заниматься или специально выделенный для этого поток, или же сами потоки симуляции, для чего их придётся периодически освобождать от задачи обработки событий.
\end{itemize*}

На рис.~\ref{fig:time-warp} приведены все компоненты схемы Time Warp. Рассмотрим подробнее процессы, происходящие при работе такой модели.

\begin{figure}[htbp]
    \centering
    \inputpicture{drawings/time-warp}
    \caption[Симуляция Time Warp]{Оптимистичная симуляция Time Warp. Точками обозначены события, подлежащие процессу сборки окаменелостей, штриховыми линиями — уже обработанные, к которым симуляция потенциально может откатиться, жирным — текущее событие, тонкой линией — запланированные в будущем. Серым фоном выделено отставшее сообщение, а двойной линией — антисообщение}
    \label{fig:time-warp}
\end{figure}

\begin{itemize*}
\item Каждый поток обрабатывает свою очередь событий, состоящую из сообщений, порождённых как им самим, так и присланными остальными потоками, в порядке возрастания меток времени. При этом продвигается значение его $LVT$, рассылаются новые сообщения, каждое из которых несёт метку времени, когда оно должно быть обработано приёмником.

\item Если при обработке очередного сообщения оно оказывается отставшим, получивший его поток начинает процесс отката. При этом он продвигает своё локальное время в обратном направлении, обрабатывая встречающиеся события <<наоборот>>, т.е. вызывая изменения, обратные записанным в них. При этом вместо обычных рассылаются антисообщения. Этот процесс происходит до тех пор, пока исходное отставшее сообщение не перестанет быть таковым, т.е. пока $t_{straggler} < LVT$. После этого события начинают вновь обрабатываться в порядке возрастания $LVT$.

\item Ни один поток по определению не имеет значение времени, меньшее $GVT$, и ни одно необработанное сообщение (в том числе те, что впоследствии окажутся отставшими) во всей симуляции не может иметь $t^{receive} < GVT$. Из этого следует, что при дальнейшей симуляции ни один поток не будет вынужден откатывать значение своего $LVT$ в прошлое дальше, чем до значения $GVT$. Таким образом, эта величина имеет принципиальный смысл и обозначает границу между свершившимся и потенциально отменяемым прошлым симуляции. В отличие от последовательной DES и консервативных схем, обработка события в оптимистичной PDES ещё не означает, что занимаемые им ресурсы можно освободить. Это происходит потому, что в дальнейшем в случае отката это же событие могут исполнить ещё раз. Лишь когда оно окажется слева от границы глобального времени, у нас есть гарантия того, что оно <<на самом деле свершилось>>.

\end{itemize*}

\section{Распределённая общая память}

Отдельно стоит упомянуть вопрос, каким способом необходимо организовывать модель памяти, общей для всех моделируемых потоков. В случае, когда симуляция исполняется на одном многопроцессорном узле с общей памятью, все хозяйские потоки могут читать и писать её общие регионы, и аппаратура следит за тем, чтобы у всех хозяйских потоков имелось единое представление о её содержимом. Симулятору остаётся следить за корректным исполнением атомарных операций (см. секцию~\ref{sec:atomics}).

Ситуация усложняется в случае распределения симуляции на несколько узлов, каждый из которых имеет собственную память, и которые объединены с помощью сети передачи данных. В этом случае необходимо использовать доступные отправки и получения сообщений между узлами для того, чтобы все симулируемые потоки имели видимость общей памяти, т.е. необходимо создание представления \textit{распределённой общей памяти} (\abbr distributed shared memory).

В литературе описаны различные решения данной задачи~\cite{Bugnion97disco:running, graphite2010}. Рассмотрим основные приёмы, описанные в них.

% (рис.~\ref{fig:dsm}).
% \begin{figure}[htbp]
%     \centering
%     \begin{tikzpicture}[>=latex, font=\small]
%     \node[align=center] (n1e) {Узел 1};
%     \node[align=center, below=0.25cm of n1e] (n1sm) {Локальная память};
%     
%     \coordinate[below=0.5cm of n1sm.west] (a1);
%     \coordinate[below=1cm of n1sm.east] (b1);
% %     \coordinate[]
%     
%     \draw (a1) rectangle (b1);
%     
%     \node[dashed, draw, fit = (n1e) (n1sm) (a1) (b1)] {};
% 
%     \end{tikzpicture}
%     \caption[Распределённая общая память]{Распределённая общая память}
%     \label{fig:dsm}
% \end{figure}

\begin{itemize*}
    \item Главная задача системы DSM — поддержка иллюзии единого пространства. Это означает, что все обращения узлов к памяти должны перехватываться и обрабатываться таким образом, чтобы исполняющиеся процессы имели консистентное представление о значениях, хранящихся в ней. С другой стороны, ненулевая вероятность того, что требуемые приложению данные находятся на другом узле, и что для их получения приходится использовать относительно медленный (по сравнению с обращениями к локальной памяти) канал передачи данных, может серьёзно ограничить производительность и масштабируемость всей системы. Таким образом, приходится использовать ухищрения, минимизирующие число таких <<дальних>> запросов.
    \item Во-первых, если некоторый регион адресов памяти используется только для чтения (например, это программный код, константы, строки), то каждый из потоков может держать у себя его локальную копию. Таким образом избегается передача сообщений по сети и связанная с ней задержка.
    \item Во-вторых, если обнаружено, что большую часть времени блок данных читается одним и тем же узлом, то разумным представляется переместить такие данные непосредственно на этот узел, т.е. провести \textit{миграцию} данных. Возможен и обратный подход — переместить само приложение ближе к данным, т.е. провести миграцию кода~\cite{em2-migration}.
    \item Наконец, если несколько узлов одновременно пишут и читают один и тот же блок, то неизбежно большинству из них придётся  обращаться к нему по сети, тогда как для одного он будет размещён локально.
    \item Используемая в системе DSM гранулярность хранения регионов памяти влияет на требуемый объём служебной информации и на издержки при передаче изменённых блоков по сети. Как правило, она выбирается равной размеру страницы физической памяти и составляет от 4 кбайт до нескольких мегабайт.
\end{itemize*}

Существуют коммерческие продукты, предназначенные для виртуализации группы вычислительных узлов, соединённых сетью, в представление единой системы с общей памятью и объединённым числом вычислительных ядер~\cite{vsmp-foundation-free, sgi-uv}. Они позволяют исполнять программы, написанные для систем с единой памятью, без изменений в их коде и логике работы.

Отметим, что, кроме общей оперативной памяти,  аналогичные подходы могут использоваться для обеспечения прозрачного распределённого доступа к другим ресурсам, например, энергонезависимому хранилищу, файловой системе~\cite{gpfs2002, lustrefs} и т.д.

\section{Балансировка скорости отдельных потоков}

Параллельная симуляция демонстрирует прирост в скорости по сравнению с последовательной, только если участвующие в ней потоки действительно выполняют полезную работу. Как мы видели раньше, препятствовать этому могут многие факторы. Для консервативных схем это, в первую очередь, блокировка процессов, ожидающих, пока остальные компоненты симуляции продвинутся в будущее. Для оптимистичных моделей — необходимость отката и повторного исполнения для частей симуляции, слишком далеко <<забежавших>> в будущее. В обоих случаях соответствующий поток не принимает участия в продвижении глобального состояния модели в будущее. Причиной этому является несбалансированность скоростей работы его самого и взаимодействующих с ним агентов. Можно заметить, что производительность параллельных алгоритмов определяется самым медленным участвующим в них процессом.

Для повышения степени равномерности скоростей потоков вводится механизм \textit{балансировки} нагрузки на них. Способ его осуществления зависит от структуры модели. Наиболее общий принцип — миграция части данных и обрабатывающих их сущностей с сильно нагруженного потока на менее занятый. Например, если в очереди событий первого потока значительно больше событий, чем у второго, разумным представляется передать часть из них так, чтобы после завершения процесса балансировки оба они имели приблизительно равные длины очередей.

Отметим, что на практике практически никогда нет возможности понять, как работа должна быть распределена, заранее, до запуска симуляции. Поэтому разработаны методы динамической балансировки, когда периодически производится оценка эффективности эксплуатации потоков, и на основании результатов принимается решение о переносе части задачи с более нагруженных на менее нагруженные потоки~\cite{DBLP:conf/pads/PeschlowHM07}.

\section{Барьерная синхронизация}

Как было показано ранее, консервативная симуляция может <<самопроизвольно>> приходить в состояние глобальной взаимоблокировки, в котором ни один из потоков не может безопасным образом обрабатывать события своей очереди без риска нарушить каузальность системы. Таким образом, симуляция при этом состоит из двух чередующихся фаз: обработка событий одним или более потоками и взаимоблокировка в ожидании сигнала к продолжению.

Некоторые протоколы симуляции подразумевают явное приведение симуляции к полной остановке в известные моменты времени — достижение \textit{барьера} синхронизации. Каждый из участвующих в работе потоков, дойдя до определённой точки в своей работе, останавливается, ожидая сигнала от остальных, что они также достигли этой точки. После того, как все сообщения получены, он возобновляет свою работу до следующего барьера.

При этом периодически возникает <<стабильное>> состояние всей системы, в котором известны состояния всех участников, в том числе их локальные значения виртуальных времён\footnote{Более точно, известно, что ни один поток не отстал в прошлое на неопределённое число шагов, т.к. все они уже достигли барьера.}. Это позволяет определить, какие из событий во всей симуляции безопасно обрабатывать. При этом значение величины предпросмотра непосредственно определяет расстояние между соседними барьерами.

К сожалению, несмотря на более понятную структуру алгоритмов, барьерные схемы могут страдать от двух проблем, негативно влияющих на их производительность.

\begin{enumerate*}
    \item Низкая степень утилизации вычислительных ресурсов. На рис.~\ref{fig:domains} показано, что длительность каждой фазы исполнения определяется самым медленным из потоков. Остальные участники, выполнив свою долю работы, вынуждены при этом простаивать до конца фазы. Проблема усиливается при несбалансированности вычислительной нагрузки на потоки. Она менее заметна, если темп их работы приблизительно одинаков.
    \item Необходимость передачи большого числа сообщений для определения момента достижения барьера может занимать значительную долю времени работы алгоритма. С ростом числа потоков это обстоятельство может уничтожить весь выигрыш от параллелизма, т.к. почти всё время будет тратиться на обслуживание барьера.
\end{enumerate*}

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/domains}
    \caption[Симуляция с барьерной синхронизацией]{Симуляция с барьерной синхронизацией. Жирными сплошными линиями показаны активные участки симуляции, штриховыми — ожидание остальных потоков без выполнения полезной работы}
    \label{fig:domains}
\end{figure}

\section{Детерминизм параллельных моделей}

\emph{Детерминистичный} (\abbr deterministic) симулятор — модель, которая в независимых своих запусках вычисляет идентичное конечное состояние моделируемой системы для любого начального состояния при условии, что исходное состояние при каждом запуске было одинаковое. Такая программа будет повторять своё поведение при последовательных запусках на каждом своём шаге, таким образом проходя через одну и ту же последовательность состояний. 

Проявление детерминизма в поведении при исследовании целевой системы является чрезвычайно полезным свойством. Так, значительно упрощается отладка гостевых приложений, потому что становится возможным установить момент, в который в них возникает ошибочное поведение, и изучить его причины. Если записана история эволюции состояния моделируемой системы, то, изменяя состояние в обратном порядке, можно создать видимость <<обращения>> течения времени (см. главу~\ref{state}, секцию~\ref{revexec}).

\subsection{Условия детерминизма}

Для последовательного симулятора DES для повторяемости исполнений достаточно соблюдения следующих условий.
\begin{enumerate*}
\item Он написан без ошибок типа <<обращение к неинициализированной памяти>>. При нарушении этого условия сложно утверждать даже о корректности симуляции, не говоря уже об её детерминистичности.
\item Все обработчики событий ведут себя повторяемым образом. Результат их исполнения не должен зависеть от работы генератора случайных чисел, состояния хозяйских файлов, изменяемых сторонними программами, сетевых пакетов реальной сети, интерактивного вмешательства пользователя и т.п. факторов.
\item Обработка событий из очереди производится в одном и том же порядке в каждом запуске симулятора.  Для событий к несовпадающими метками времени он диктуется корректностью самого алгоритма. В случае совпадения меток у двух или более событий необходимо упорядочить их повторяемым между отдельными запусками способом. Поскольку последовательная программа создаёт такие события одно за другим, естественный способ состоит в том, чтобы использовать детерминированный порядок их создания для расстановки приоритетов при обработке в случае совпадения меток времени.
\end{enumerate*}

Последовательная симуляция общего назначения, не использующая в свой работе вероятностные механизмы (метод Монте Карло и т.п.), должна обеспечивать детерминизм, иначе возникают законные вопросы об её корректности.

\begin{digression}
Необходимо отметить, что в окружающем нас мире детерминизм не такое частое явление. Механистическая картина мира, в которой частицы движутся по законам Ньютона, не в силах сойти c предопределённых им начальным состоянием траекторий, оказалась неприменимой в масштабах микро-, а значит и макромира. На смену ей пришла квантовая механика~\cite{ivanov-theorphys}. По этой причине следует понимать, что модели, описываемые в данной книге, имеют далеко не универсальную применимость.

Кроме того, как будет показано дальше, выполнение условий для повторяемости параллельной симуляции негативным образом влияет на её производительность, так как требует дополнительной синхронизации потоков. Наконец, ограничивая модель многоагентной системы в её поведении только одним сценарием из множества реально допустимых, мы лишаем себя возможности наблюдать их проявления и как-то повлиять на них. Например, последовательная модель многопроцессорной системы часто неспособна воспроизвести проблемы гонок данных в программах, наблюдаемых на реальной аппаратуре.

По этим причинам нельзя с определённостью заявлять, что детерминизм для параллельной симуляции является обязательным или приоритетным свойством во всех случаях.

\end{digression}

\subsection{События с одинаковой меткой времени}

Для параллельной симуляции ситуация несколько сложнее. Три сформулированные выше условия должны выполняться для каждого участвующего в ней потока. Возникает новой источник неопределённости — события с одинаковой меткой времени могут приходить с сообщениями от других потоков, при этом из-за различий в скоростях их исполнения в разных запусках порядок создания может быть различным (рис.~\ref{fig:same-time-stamp}). 

\begin{figure}[htbp]
    \centering
	\inputpicture{drawings/same-time-stamp}
    \caption[Различный порядок получения сообщений от внешних потоков]{Различный порядок получения сообщений от внешних потоков}
    \label{fig:same-time-stamp}
\end{figure}

Каким образом можно детерминистичным образом упорядочить обработку событий в данных условиях?

\begin{description*}
    \item[Определённый порядок получения событий.] Можно попытаться получать события от внешних потоков в строго фиксированном порядке. Например, в модели, содержащей три процесса, первый всегда ожидает сообщения от второго потока, прежде чем перейти к получению данных от третьего. Это устраняет указанный выше источник неопределённости, однако накладывает серьёзные ограничения на характер коммуникаций в системе, т.к. неосторожное использование операции блокирующего чтения может привести к взаимоблокировке.
    \item[Расширение метки времени дополнительными битами точности.] В данном подходе принимаются меры, чтобы времена событий никогда полностью не совпадали. Для этого метка времени должна содержать дополнительные <<младшие>> биты, уникальные для всех событий во всей симуляции. При этом для любых двух симулируемых событий расширенные таким образом метки времени никогда не могут совпасть. Уникальные младшие биты могут формироваться на основе двух чисел: порядкового номера создаваемого события, монотонно возрастающего для каждого потока-отправителя, и порядкового номера создающего его хозяйского потока. Альтернативный подход заключается в использовании генератора псевдослучайных чисел, инициируемого одним и тем же начальным значением так, что производимая им последовательность одинакова при всех запусках симуляции\footnote{Отметим, что построение генератора, пригодного для детерминистичной многопоточной генерации псевдослучайных чисел, нетривиально~\cite{dotmix}.}.
\end{description*}

% Если в системе не гарантирован предпросмотр сообщений, больше нуля, то при использовании механизма передачи сообщений без подтверждений (\abbr acknowledgement) возникает ещё одно затруднение, даже если ни одни

\subsection{Домены синхронизации}

Рассмотрим пример использования консервативного алгоритма с барьерной синхронизацией, применимого в ситуациях, когда в симуляции  можно выделить области, характерная частота коммуникаций между которыми превышает частоту коммуникаций внутри каждой из них, а доставка сообщений может быть задержана до барьера. Такие части симуляции, сообщения между которыми подвергаются задержке, называются \textit{домены синхронизации}~\cite{simics-accelerator-guide}.  Для подобной схемы организации должны выполняться следующие условия.

\begin{enumerate*}
\item Все домены могут работать параллельно и независимо до тех пор, пока не достигнут границы текущей квоты симулируемого времени, где они останавливаются на синхронизационном барьере, ожидая, пока все остальные домены также достигнут его.

\item Устройства, требующие частых коммуникаций или зависящие от точного моделирования вариаций длительностей задержек, не могут быть разнесены в разные домены без дополнительных ухищрений. Например, информация об изменениях в общей памяти всегда будет задерживаться от момента записи до момента междоменной синхронизации, когда может оказаться, что в симуляции произошло каузальное нарушение. В таком случае остаётся использовать откат некоторых или всех конфликтующих потоков с их повторным исполнением до тех пор, пока все потоки не достигнут барьера непротиворечивым способом. Т.е. приходится применять оптимистичный подход.
\end{enumerate*}

На практике при симуляции многомашинных конфигураций в составе одного домена размещается один гостевой компьютер, а моделирование передачи сетевых пакетов (Ethernet, Infiniband и т.п.) производится через описанную схему. При этом достигается наилучший баланс между скоростью симуляции (т.к. характерные задержки сетевых устройств больше, чем наблюдаемые между, скажем, памятью и процессором) и её точностью (протоколы сетевого взаимодействия толерантны к большим задержкам и высокой неопределённости времени доставки пакетов). Однако с ростом скоростей (для современных серверных и HPC сетевых карт скорость доставки пакетов приближается к скорости работы локальных жёстких дисков) квант синхронизации уменьшается, что приводит к повышению частоты организации барьеров с  соответствующим падением скорости симуляции.

Описанная система доменов может быть вложенной, т.е. внутри одного домена определены несколько меньших, синхронизация которых также происходит с фиксированной задержкой. При этом интервал синхронизации поддоменов должен быть короче используемого для их внешнего, <<родительского>>  домена.

Поскольку порядок передачи сообщений между потоками симулятора при достижении барьера контролируется центральным агентом, он достаточно просто может проводить его повторяемым между симуляциями образом, что позволяет сделать доменную схему детерминистичной.

% \subsection{Симуляторы и гонки данных в гостевых программах}
% \todo
% 
% Как уже было отмечено ранее, в последовательной модели многопроцессорной системы гонки данных маскируются тем обстоятельством, что в любой момент времени общую память может изменять только один поток. Этот эффект может быть нежелательным при симуляции.
% 


\section{Параллельная симуляция одного процессора}

У наблюдательного читателя может возникнуть вопрос об <<обратной>> ситуации: можно ли получить выигрыш от многих ядер хозяйской системы, если требумое число симулируемых ядер при этом значительно меньше?  То есть — возможно ли каким-то образом ускорить симуляцию одного гостевого процессора, используя для этого несколько параллельных хозяйских потоков? 

Ответ зависит от того, на каком уровне абстракции лежит модель. Для уровня архитектуры (набора инструкций) ответ почти всегда отрицательный. Эффективное решение этой задачи — параллельное исполнение нескольких машинных инструкций последовательной программы — равносильно построению \textit{быстрой} модели внеочередного исполнения (\abbr out of order execution), способной <<на лету>> находить независимые по данным инструкции и исполнять их. Весь эффект от параллельного исполнения при этом будет нивелирован длительным анализом. Если даже упростить задачу, позволив проводить статический, а не динамический анализ, мы сводим её к проблеме построения автопараллелизующего компилятора. Как известно, далеко не всякий код подвержен такому преобразованию.

Значительный потенциал к параллелизации модели процессора возникает при переходе к потактовым моделям. На уровне симуляции представления синхронной цифровой схемы, состоящей из множества узлов, каждый шаг выполняющих свои фиксированные функции на данных, полученных с предыдущего шага работы, становится возможным запускать большое число компонент схемы параллельно. При этом меняются принципы организации симуляторов для таких моделей. Мы рассмотрим их подробнее в главе~\ref{cycle}.

\section{Заключение}

В данной главе были обрисованы лишь основные принципы организации симуляции, использующей более одного потока хозяйской системы. Конечно, за десятки лет исследований в этой области были созданы многочисленные варианты многопроцессорных, параллельных и распределённых решений. За границами данного описания остались многие важные вопросы, в том числе следующие.

\begin{itemize*}
\item Оптимизации, используемые для минимизации числа необходимых синхронизаций в консервативных моделях. 
\item Решения, позволяющие уменьшить частоту и стоимость откатов в схемах оптимистичных. 
\item Потребление памяти параллельным симулятором по сравнению с последовательной версией.
\item Построение консервативных и оптимистичных схем с нулевым предпросмотром, и проблематика обеспечения детерминизма для этого случая.
\item Эффективное взаимодействие планировщика потоков хозяйской системы и симуляции.
\item Проблема учёта событий, уже отправленных, но ещё не полученных, т.е. находящихся <<в полёте>>.
\item Способы эффективного вычисления $GVT$, организации барьеров, создания точек сохранения и другие распределённые алгоритмы, необходимые для нормальной работы параллельного симулятора.
\end{itemize*}

Интересующийся читатель может найти подробное описание этих и других вопросов распределённой симуляции в~\cite{fujimoto-parallel-dist-sim}. 
Тем не менее, хотелось бы ответить на два важных вопроса, связанных с основной темой данной главы: почему именно параллельный симулятор так сложно построить? и если он таки построен, то возможно ли получить выигрыш в скорости?

\paragraph{Почему параллельная симуляция настолько сложна?}\label{sec:why-pdes-hard}

В работе~\cite{Fujimoto-pdes} в секции <<Why PDES is hard?>> приводится достаточно краткий и одновременно подробный ответ на этот вопрос. Приведём свободный перевод: \textit{<<\dots необходимо определить, может или нет сообщение $E_1$ быть обработано одновременно с $E_2$. Но каким образом узнать, влияет или нет $E_1$ на $E_2$, без его симуляции?>>} 

Переформулируем это высказывание следующим образом: в отличие от других параллельных приложений, в которых зависимости между потоками статически определены и известны ещё до стадии исполнения, алгоритмы симуляции не определяют своё поведение полностью. Значительная часть его содержится в подлежащей симуляции программе, про которую в общем случае ничего не известно. 

\paragraph{Потолок ускорения от параллелизма.} Верхнюю границу значения ускорения, получаемого в случае использования параллельного симулятора вместо последовательного, устанавливает степень параллелизма, проявляемого гостевым приложением. Никакой параллельный симулятор не будет в состоянии ускорить код, каждый этап исполнения которого зависит от предыдущего. Поэтому, прежде чем вкладывать усилия в трудоёмкий процесс написания параллельных моделей, необходимо определить, способен ли целевой класс гостевых приложений использовать весь создаваемый потенциал.

\input{parallel-questions}

\iftoggle{webpaper}{
    \printbibliography[title={Литература}]
}{}
